
\section{Dimension reduction (unsup. reg)}
    \textbf{PCA:} Compress data with low dim. representation\\
    $(\boldsymbol{w}^*,\boldsymbol{z}^*)={\underset{||\boldsymbol{w}||^2_2=1,\boldsymbol{z}}{\mathrm{argmin}}} \sum_{i=1}^n ||z_i\boldsymbol{w}-\boldsymbol{x}_i||^2_2 $, 
    $z_i^*=\boldsymbol{w}^T\boldsymbol{x}_i$, 
    $\boldsymbol{w}^*=\underset{||\boldsymbol{w}||^2_2=1}{\mathrm{argmin}}\ \sum_{i=1}^n ||\boldsymbol{ww}^T\boldsymbol{x}_i - \boldsymbol{x}_i||^2$
    $=\underset{||\boldsymbol{w}||^2_2=1}{\mathrm{argmax}} \sum_i^{n} (\boldsymbol{w}^T \boldsymbol{x}_i) ^2$\quad if $\boldsymbol{w}$ is a 1-D vector.(k=1)\\

    
    $(\boldsymbol{W},\boldsymbol{z}_1,...,\boldsymbol{z}_n)=\underset{\boldsymbol{W}^T\boldsymbol{W}=\boldsymbol{I}_k,\boldsymbol{z}}{\mathrm{argmin}} \sum_i^n ||\boldsymbol{Wz}_i-\boldsymbol{x}_i||^2_2 $\\
    with orthogonal $\boldsymbol{W} = [\boldsymbol{v}_1| ... | \boldsymbol{v}_k] \in \mathbb{R}^{d \times k}$ and $\boldsymbol{z}_i = \boldsymbol{W}^T\boldsymbol{x}_i$ where $\boldsymbol{v}_{1...k}$ are the first k columns of $\boldsymbol{V}\ (SVD: \boldsymbol{X}= \boldsymbol{USV}^T)$\\
    \textit{Empirical Covariance:} $\boldsymbol{\Sigma} = \frac{1}{n}\sum_i \boldsymbol{x}_i\boldsymbol{x}_i^T $
    \makebox[0.5cm][]{}with Eigenvalues $\lambda_1\geq \lambda_2 \geq ... \Rightarrow$ $\boldsymbol{\Sigma} = \sum_i \lambda_i \boldsymbol{v}_i\boldsymbol{v}_i^T$ \\
    $\frac{1}{n}\underset{\boldsymbol{W}^T\boldsymbol{W}=\boldsymbol{I}_k}{\mathrm{argmin}} \sum ||\boldsymbol{Wz}_i-\boldsymbol{x}_i||^2_2 = \sum_{i=k+1}^d \lambda_i$
    
    \textbf{Kernel PCA:} $w = \sum_{j=1}^n {\alpha}^{(j)} \phi(x_j)$\\
    \centerline{$\boldsymbol{\alpha}^*= \underset{\boldsymbol{\alpha}^T\boldsymbol{K\alpha}=1}{\mathrm{argmax}}\; \boldsymbol{\alpha}^T\boldsymbol{K}^T\boldsymbol{K}\boldsymbol{\alpha} = \underset{\boldsymbol{\alpha}}{\mathrm{argmax}} \frac{\boldsymbol{\alpha}^T\boldsymbol{K}^T\boldsymbol{K}\boldsymbol{\alpha}}{\boldsymbol{\alpha}^T\boldsymbol{K}\boldsymbol{\alpha}}$}
    with solution $\alpha^{(i)} = \frac{1}{\sqrt{\lambda_i}}v_i$ from $K=\sum \lambda_iv_iv_i^T$\\
    A new point x is projected by: $z_i = \sum_{j=1:n} \alpha_j^{(i)} k(x_j,x)$
    
    \textbf{Autoencoders} initialization matters\\ 
    $f(x,\theta)=f_{dec}(f_{enc}(x;\theta_{enc});\theta_{dec})\quad \mathbb{R}^{d}\to \mathbb{R}^{k<d} \to \mathbb{R}^{d}$
    If linear activ. func., AE (non-conv.) equivalent to PCA.